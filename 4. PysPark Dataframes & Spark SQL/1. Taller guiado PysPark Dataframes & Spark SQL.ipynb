{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.11:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Processing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7f5439ff10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Data Processing\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DataFrame\n",
    "Un DataFrame en el contexto de Apache Spark es una estructura de datos distribuida que organiza los datos en forma de columnas, similar a una tabla en una base de datos relacional. Sin embargo, los DataFrames en Spark presentan algunas características distintivas que los hacen especialmente adecuados para el procesamiento de grandes volúmenes de datos en clústeres:\n",
    "\n",
    "* **Distribución y paralelismo:** Los DataFrames en Spark se dividen en particiones, lo que permite que los datos se distribuyan de manera eficiente en un clúster de máquinas. Cada partición se procesa en paralelo en diferentes nodos del clúster, lo que mejora significativamente el rendimiento y la escalabilidad del procesamiento de datos.\n",
    "\n",
    "* **Optimización de consultas:** Spark utiliza su motor de optimización Catalyst para mejorar el rendimiento de las operaciones en DataFrames. Catalyst optimiza el plan de ejecución de las consultas, reorganizando y reescribiendo las operaciones para minimizar la cantidad de datos que se transfieren a través de la red y maximizar la eficiencia de la computación distribuida.\n",
    "\n",
    "* **Tipado de datos:** A diferencia de los RDDs (Resilient Distributed Datasets), los DataFrames en Spark tienen un esquema de datos definido. Esto significa que cada columna tiene un tipo de dato asociado, lo que facilita la detección temprana de errores y permite que Spark realice optimizaciones de rendimiento específicas basadas en el tipo de datos.\n",
    "\n",
    "* **Lenguajes de programación compatibles:** Los DataFrames en Spark están disponibles en varios lenguajes de programación, como Scala, Java, Python y R. Esto facilita la adopción y el uso de Spark por parte de una amplia variedad de desarrolladores.\n",
    "\n",
    "* **Integración con fuentes de datos diversas:** Spark ofrece conectores para una amplia gama de fuentes de datos, incluyendo sistemas de almacenamiento como HDFS, bases de datos relacionales, fuentes de datos en tiempo real como Kafka y sistemas de almacenamiento en la nube como AWS S3 y Azure Blob Storage. Esto permite que los DataFrames en Spark sean una herramienta versátil para el procesamiento de datos desde diversas fuentes.\n",
    "\n",
    "* **API rica y funcionalidades avanzadas:** Los DataFrames en Spark proporcionan una API rica con una amplia gama de operaciones de transformación y acción que permiten realizar tareas de procesamiento de datos complejas, como filtrado, agrupación, ordenación y agregación, de manera eficiente.\n",
    "\n",
    "Esto convierte los DataFrame de Spark en una herramienta esencial para el análisis y procesamiento de datos a gran escala en clústeres de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Crear un DataFrame\n",
    "Existen dos formas de crear un DataFrame:  a través de un RDD (Resilient Distributed Dataset) existente o mediante la lectura de datos desde un archivo externo, como un archivo CSV, JSON, Parquet u otros formatos compatibles.\n",
    "### 1.1.1. RDD\n",
    "Un RDD es una estructura de datos fundamental en Spark que representa una colección distribuida de elementos. Puedes crear un DataFrame a partir de un RDD existente utilizando la función createDataFrame() en Spark. Esto es útil cuando ya tienes datos en forma de RDD y deseas aprovechar las ventajas adicionales que ofrece un DataFrame, como la optimización de consultas y el esquema de datos.\n",
    "\n",
    "En el siguiente código, crearemos un conjunto de datos llamado 'datos' que consiste en una lista de tuplas. Cada tupla representa información sobre un empleado, incluyendo su ID, nombre, edad, género, ocupación y salario. Luego de esto, emplearemos Spark para paralelizar este conjunto de datos en un RDD llamado 'empleadosRDD'. Finalmente, se ejecuta la operación 'collect()' en 'empleadosRDD' para recuperar todos los elementos del RDD y mostrarlos como una lista en la salida. Esto permitirá ver los datos de los empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empleadosRDD type: <class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'Miguel', 21, 'Masculino', 'Chef', 6000000),\n",
       " (1, 'Juan', 33, 'Masculino', 'Ingeniero', 4500000),\n",
       " (2, 'Ana', 38, 'Femenino', 'Arquitecta', 6200000),\n",
       " (3, 'Carmen', 52, 'Femenino', 'Abogada', 7500000)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos=[(0, \"Miguel\", 21, \"Masculino\", \"Chef\", 6000000),\\\n",
    "       (1,\"Juan\",33,\"Masculino\",\"Ingeniero\",4500000),\\\n",
    "       (2,\"Ana\",38,\"Femenino\",\"Arquitecta\",6200000),\\\n",
    "       (3,\"Carmen\",52,\"Femenino\",\"Abogada\",7500000)]\n",
    "empleadosRDD=sc.parallelize(datos)\n",
    "\n",
    "print('\\nempleadosRDD type:', type(empleadosRDD))\n",
    "\n",
    "empleadosRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un DataFrame de Spark llamado 'empleadosDF' a partir del RDD empleando diferentes métodos como ejemplo. La función createDataFrame() de Spark toma un RDD como entrada y crea un DataFrame con una estructura de columnas basada en los datos del RDD. Al igual que en el caso anterior, el collect() en 'empleadosDF' nos permitirá recuperar todos los datos del DataFrame y mostrarlos en la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y6Va4MWkk8vO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empleadosDF type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='Miguel', _3=21, _4='Masculino', _5='Chef', _6=6000000),\n",
       " Row(_1=1, _2='Juan', _3=33, _4='Masculino', _5='Ingeniero', _6=4500000),\n",
       " Row(_1=2, _2='Ana', _3=38, _4='Femenino', _5='Arquitecta', _6=6200000),\n",
       " Row(_1=3, _2='Carmen', _3=52, _4='Femenino', _5='Abogada', _6=7500000)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF=spark.createDataFrame(empleadosRDD)\n",
    "\n",
    "print('\\nempleadosDF type:', type(empleadosDF))\n",
    "\n",
    "empleadosDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE2fj7sWlsaK"
   },
   "source": [
    "---\n",
    "La salida que observamos en la anterior celda no es la más amigable para la vista, sin embargo, los DataFrame tienen algunos métodos que facilitan la operación y visualización de los datos como mostraremos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mraoVSg6lzaZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+-------+\n",
      "| _1|    _2| _3|       _4|        _5|     _6|\n",
      "+---+------+---+---------+----------+-------+\n",
      "|  0|Miguel| 21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan| 33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana| 38| Femenino|Arquitecta|6200000|\n",
      "|  3|Carmen| 52| Femenino|   Abogada|7500000|\n",
      "+---+------+---+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Ejemplo 2**\n",
    "\n",
    "Para comprender mejor los datos podemos indicar el nombre de las columnas al momento de crear el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7FglWofnm_eZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=0, Nombre='Miguel', Edad=21, Sexo='Masculino', Profesion='Chef', Salario=6000000),\n",
       " Row(Id=1, Nombre='Juan', Edad=33, Sexo='Masculino', Profesion='Ingeniero', Salario=4500000),\n",
       " Row(Id=2, Nombre='Ana', Edad=38, Sexo='Femenino', Profesion='Arquitecta', Salario=6200000),\n",
       " Row(Id=3, Nombre='Carmen', Edad=52, Sexo='Femenino', Profesion='Abogada', Salario=7500000)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  3|Carmen|  52| Femenino|   Abogada|7500000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF=spark.createDataFrame(empleadosRDD, schema=[\"Id\",\"Nombre\",\"Edad\",\"Sexo\",\"Profesion\",\"Salario\"])\n",
    "\n",
    "display(empleadosDF.collect())\n",
    "\n",
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLOse8lh7bnb"
   },
   "source": [
    "Veamos qué tipo de dato se adoptó para cada variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Pi1tA6e27QsN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Edad: long (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Profesion: string (nullable = true)\n",
      " |-- Salario: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Ejemplo 3**\n",
    "\n",
    "Como obervamos en el esquema anterior \"Id\", \"Edad\" y \"Salario\" los estamos usando con el mismo tipo de dato (long), pero podríamos optimizar el entorno si modificamos este tipo para las variables. Para esto, podemos definir explícitamente el esquema de datos del DataFrame utilizando la estructura 'StructType' y 'StructField'. Esto se hace para controlar el tipo de datos de cada columna en el DataFrame.\n",
    "\n",
    "La estructura 'StructType' se utiliza para definir el esquema del DataFrame, especificando las columnas y sus tipos de datos correspondientes. En este caso, se definieron las seis columnas (\"Id\", \"Nombre\", \"Edad\", \"Sexo\", \"Profesion\" y \"Salario\") con sus respectivos tipos de datos.\n",
    "\n",
    "Ver más sobre tipo de datos en: [Supported Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NWlw0B_j7mVl"
   },
   "outputs": [],
   "source": [
    "empleadosDF=spark.createDataFrame(empleadosRDD, \\\n",
    "          StructType([ \\\n",
    "                      StructField(\"Id\", ByteType(),False), \\\n",
    "                      StructField(\"Nombre\", StringType(),False), \\\n",
    "                      StructField(\"Edad\", ByteType(),False), \\\n",
    "                      StructField(\"Sexo\", StringType(),False), \\\n",
    "                      StructField(\"Profesion\", StringType(),False), \\\n",
    "                      StructField(\"Salario\", IntegerType(),False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vf0FHe2a-JAs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: byte (nullable = false)\n",
      " |-- Nombre: string (nullable = false)\n",
      " |-- Edad: byte (nullable = false)\n",
      " |-- Sexo: string (nullable = false)\n",
      " |-- Profesion: string (nullable = false)\n",
      " |-- Salario: integer (nullable = false)\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  3|Carmen|  52| Femenino|   Abogada|7500000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.printSchema()\n",
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando con el esquema previamente mostrado (**Ejemplo 2**), se observa que la diferencia principal es que aquí se ha definido explícitamente el tipo de datos de las columnas \"Id\" y \"Edad\" como 'ByteType', y \"Salario\" como 'IntegerType', en lugar de 'long'. Esto permite un mayor control sobre el esquema y potencialmente ahorra espacio de almacenamiento si los valores de estas columnas son de un tamaño menor a los que se manejan en 'long'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HkDAZjMUkoL"
   },
   "source": [
    "### 1.1.2. Archivo Externo\n",
    "\n",
    "**Cargar archivo csv**\n",
    "\n",
    "Ahora emplearemos la función spark.read.csv() para cargar datos desde un archivo CSV y crear el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NRU4vYr4dOJe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empleadosDF type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF = spark.read.csv(\"data/empleados.csv\")\n",
    "\n",
    "print('\\nempleadosDF type:', type(empleadosDF))\n",
    "\n",
    "empleadosDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Sw1jJpZAXoBe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "|_c0|   _c1| _c2|      _c3|       _c4|    _c5|\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybaKS5i_YUDf"
   },
   "source": [
    "Como se observa, el DataFrame está creado correctamente pero no le hemos indicado que la primera línea es el header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YotjcPVqYche"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF = spark.read.option(\"header\",True).csv(\"data/empleados.csv\")\n",
    "\n",
    "empleadosDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zFo1H8ynYxZX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Profesion: string (nullable = true)\n",
      " |-- Salario: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wriIifL4YuK6"
   },
   "source": [
    "Al igual que el ejemplo presentado con anterioridad, podemos ver y asignar el formato a cada variable al momento de crear el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sfggLbPOZAOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: byte (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Edad: byte (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Profesion: string (nullable = true)\n",
      " |-- Salario: integer (nullable = true)\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"Id\",ByteType(),True) \\\n",
    "      .add(\"Nombre\",StringType(),True) \\\n",
    "      .add(\"Edad\",ByteType(),True) \\\n",
    "      .add(\"Sexo\",StringType(),True) \\\n",
    "      .add(\"Profesion\",StringType(),True) \\\n",
    "      .add(\"Salario\",IntegerType(),True)\n",
    "\n",
    "empleadosDF = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option(\"delimiter\", \",\") \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"data/empleados.csv\")\n",
    "\n",
    "empleadosDF.printSchema()\n",
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8vIOZ6_Uwen"
   },
   "source": [
    "**Cargar archivo JSON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función spark.read.json() nos permite leer un archivo JSON en un DataFrame de Spark. Observaremos el tipo y el esquema del JSON cargado y asignaremos el formato a las variables como en los casos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "G61hk7eNcnnW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empleadosDF type: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "root\n",
      " |-- Edad: long (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Profesion: string (nullable = true)\n",
      " |-- Salario: long (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      "\n",
      "+----+---+------+----------+-------+---------+\n",
      "|Edad| Id|Nombre| Profesion|Salario|     Sexo|\n",
      "+----+---+------+----------+-------+---------+\n",
      "|  33|  1|  Juan| Ingeniero|4500000|Masculino|\n",
      "|  38|  2|   Ana|Arquitecta|6200000| Femenino|\n",
      "+----+---+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF = spark.read.json(\"data/empleados.json\")\n",
    "\n",
    "print('\\nempleadosDF type:', type(empleadosDF), '\\n')\n",
    "\n",
    "empleadosDF.printSchema()\n",
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tQ1W0c11gUI1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empleadosDF type: <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Edad: byte (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Profesion: string (nullable = true)\n",
      " |-- Salario: integer (nullable = true)\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF = spark.read.json(\"data/empleados.json\", \\\n",
    "            StructType([ \\\n",
    "                      StructField(\"Id\", IntegerType(),False), \\\n",
    "                      StructField(\"Nombre\", StringType(),False), \\\n",
    "                      StructField(\"Edad\", ByteType(),False), \\\n",
    "                      StructField(\"Sexo\", StringType(),False), \\\n",
    "                      StructField(\"Profesion\", StringType(),False), \\\n",
    "                      StructField(\"Salario\", IntegerType(),False)]))\n",
    "\n",
    "print('\\nempleadosDF type:', type(empleadosDF), '\\n')\n",
    "\n",
    "empleadosDF.printSchema()\n",
    "\n",
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas información sobre carga de datos en: [Data Sources Spark](https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwsMNDa0szmb"
   },
   "source": [
    "## 1.2 Acceso a los datos\n",
    "Una gran ventaja de trabajar con DataFrames es la capacidad de acceder a cada variable o columna de manera intuitiva y conveniente. En general, los DataFrames están diseñados para imitar la funcionalidad de las tablas en una base de datos relacional y esto significa que puedes acceder a las columnas ¡de la misma manera que accederías a los campos de una tabla de base de datos: a través del nombre del DataFrame, seguido del carácter punto (.) y el nombre de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PG0c_ePStC6V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Salario'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF.Salario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTzhbt-M2STc"
   },
   "source": [
    "## 1.3. Operaciones\n",
    "Los DataFrames en Spark permiten realizar una amplia variedad de operaciones, que se pueden dividir en dos categorías principales: propiedades y métodos. \n",
    "### 1.3.1. Propiedades\n",
    "Las propiedades son atributos o características de un DataFrame que se pueden consultar para obtener información sobre su estructura y contenido. Algunas propiedades comunes incluyen:\n",
    "\n",
    "##### columns\n",
    "\n",
    "Esta propiedad devuelve una lista de los nombres de las columnas en el DataFrame. Te permite conocer las columnas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Be1A4LgAzMEf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'Nombre', 'Edad', 'Sexo', 'Profesion', 'Salario']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faanWpPs2BbY"
   },
   "source": [
    "##### dtypes\n",
    "Muestra los tipos de datos de cada columna en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "azYvdFGL2LyJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 'int'),\n",
       " ('Nombre', 'string'),\n",
       " ('Edad', 'tinyint'),\n",
       " ('Sexo', 'string'),\n",
       " ('Profesion', 'string'),\n",
       " ('Salario', 'int')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKuiuMAAJ9qi"
   },
   "source": [
    "##### rdd\n",
    "Convierte un DataFrame a un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2cCsvo22KEDP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empRDD = empleadosDF.rdd\n",
    "type(empRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oP-2EUnh2Fq"
   },
   "source": [
    "\n",
    "### 1.3.2. Métodos\n",
    "Los métodos son funciones que se aplican al DataFrame para realizar operaciones de transformación, filtrado, agregación y otras tareas de procesamiento de datos. Observemos cómo se emplean métodos comunes en el siguiente DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_jzgLx4RMO_k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleadosDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### agg(funciones_agregacion)\n",
    "\n",
    "Este método realiza una agregación sobre el data frame, es decir que a partir del data frame original, devuelve un nuevo data frame que contiene el resultado de las funciones de agregación realizadas sobre el conjunto de datos.\n",
    "\n",
    "Estas funciones de agregación son especificadas por el usuario y hacen referencia a operaciones básicas sobre los datos. Estas son algunas de las funciones de agregación disponibles:\n",
    "\n",
    "* first: entrega el primer elemento\n",
    "* last: entrega el último elemento\n",
    "* min: entrega el valor mínimo de una variable específica\n",
    "* max: entrega el valor máximo de una variable específica\n",
    "* sum: entrega la suma de los valores de una variable específica\n",
    "* avg: entrega el promedio de los valores de una variable específica\n",
    "\n",
    "Usemos el método agg para conocer el promedio de los salarios de los empleados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TtWcavRKVLAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(avg(Salario)=5350000.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salario_mean = empleadosDF.agg({\"Salario\": \"avg\"}).take(1)\n",
    "\n",
    "print(type(salario_mean))\n",
    "\n",
    "salario_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IDl1NOjjua7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type salario_mean:  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+------------+\n",
      "|avg(Salario)|\n",
      "+------------+\n",
      "|   5350000.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salario_mean=empleadosDF.agg({\"Salario\": \"avg\"})\n",
    "\n",
    "print('Type salario_mean: ', type(salario_mean))\n",
    "\n",
    "salario_mean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xua_pWVowwTr"
   },
   "source": [
    "También podemos utilizar las funciones disponibles en la clase functions de la librería pyspark.sql. Por ejemplo, empleemos algunas de las funciones para hallar el máximo, el mínimo y el promedio de \"Edad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HDct3eNTxCls"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type edad_stats:  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------+---------+---------+\n",
      "|min(Edad)|max(Edad)|avg(Edad)|\n",
      "+---------+---------+---------+\n",
      "|       33|       38|     35.5|\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "edad_stats = empleadosDF.agg( \\\n",
    "    functions.min(empleadosDF.Edad), \\\n",
    "    functions.max(empleadosDF.Edad), \\\n",
    "    functions.avg(empleadosDF.Edad))\n",
    "\n",
    "print('Type edad_stats: ', type(edad_stats))\n",
    "\n",
    "edad_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iz-emL9Qyihu"
   },
   "source": [
    "En este caso utilizamos tres funciones de agregación, el resultado es un nuevo DataFrame de una sola fila y tres columnas (una por cada función de agregación)\n",
    "##### corr(var1,var2)\n",
    "Este método se utiliza para calcular la correlación entre un par de variables del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "S1I6KZeGzqox"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF.corr(\"Edad\", \"Salario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kFop7Ss02ur"
   },
   "source": [
    "##### count()\n",
    "Se utiliza para conocer el número de registros (filas) en el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eat8zmt_0_Qo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empleadosDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### union(other)\n",
    "Este método crea un nuevo DataFrame a partir de la unión de dos DataFrames. Para ejemplificar el método, actualicemos nuestro DataFrame a uno nuevo (emp_act) con el registro de otros empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+-------+\n",
      "| _1|    _2| _3|       _4|        _5|     _6|\n",
      "+---+------+---+---------+----------+-------+\n",
      "|  0|Miguel| 21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana| 40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis| 62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel| 30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana| 38| Femenino|Arquitecta|6200000|\n",
      "+---+------+---+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuevos_emp = spark.createDataFrame([ \\\n",
    "    (0, \"Miguel\", 21, \"Masculino\", \"Chef\", 6000000),\\\n",
    "      (4,\"Ana\",40,\"Femenino\",\"Docente\",7600000),\\\n",
    "      (5,\"Luis\",62,\"Masculino\",\"Panadero\",None),\\\n",
    "    (6,\"Daniel\",30,\"Masculino\",\"Empresario\",9000000),\\\n",
    "    (2,\"Ana\",38,\"Femenino\",\"Arquitecta\",6200000)])\n",
    "\n",
    "nuevos_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_act = empleadosDF.union(nuevos_emp)\n",
    "\n",
    "emp_act.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Mh_1p_81Zqp"
   },
   "source": [
    "##### distinct()\n",
    "Crea un nuevo DataFrame a partir de los registros (filas) que sean diferentes en el DataFrame inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Bh6Z541z1qpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dist = emp_act.distinct()\n",
    "emp_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivRGC3i03UHp"
   },
   "source": [
    "##### drop(var)\n",
    "Toma el DataFrame inicial, elimina la variable (columna) especificada y entrega un nuevo DataFrame con el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "W4PXJtHx3nTY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+----------+-------+\n",
      "|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+------+----+---------+----------+-------+\n",
      "|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_drop = emp_dist.drop(\"Id\")\n",
    "emp_drop.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHI72SS07IoB"
   },
   "source": [
    "##### dropDuplicates(var)\n",
    "Este método devuelve un nuevo DataFrame en el que ha eliminado los registros (filas) duplicadas. Esto ya los hacía el método distinct(), la diferencia es que en dropDuplicates() se puede especificar las columnas a evaluar para considerar que dos registros son duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Fd_8g2ZU7vzt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_drop_id = emp_act.dropDuplicates([\"Id\"])\n",
    "emp_drop_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKLxU8-0_Ul0"
   },
   "source": [
    "##### fillna(valor,col[ ])\n",
    "Este método devuelve un nuevo DataFrame luego de sustituir los valores nulos detectados por el valor especificado. Adicionalmente, es posible indicarlo cuales son las columnas sobre las que va a operar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SenCa9GU_ypO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|1000000|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_fna = emp_act.fillna(1000000)\n",
    "emp_fna.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e_CYXyeAdvX"
   },
   "source": [
    "##### dropna(_cuantas, _umbral, _columnas)\n",
    "Este método devuelve un nuevo DataFrame resultado de eliminar los registros (filas) que contienen nulos en el DataFrame inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5lov_pBYA5dO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dna = emp_act.dropna(\"any\")\n",
    "emp_dna.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV2_amAeCA6g"
   },
   "source": [
    "##### filter(condicion)\n",
    "Funciona similar al filter de RDD. Devuelve un nuevo DataFrame que contiene los registros para los cuales la condición arroja como resultado True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FECz5ikFCMIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empf = emp_act.filter(emp_act.Salario>5000000)\n",
    "empf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP1at8fIES2H"
   },
   "source": [
    "##### GroupBy(var)\n",
    "Genera un nuevo DataFrame donde agrupa los registros que coincidan en la variable especificada. Luego de agrupar es posible realizar agregaciones para indicar que se desea realizar con los valores de los registros agrupados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SfFC-Wr-EVDu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Inicial\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n",
      "DataFrame promediando todas las variables\n",
      "\n",
      "+------+------------------+------------------+-----------------+\n",
      "|Nombre|           avg(Id)|         avg(Edad)|     avg(Salario)|\n",
      "+------+------------------+------------------+-----------------+\n",
      "|   Ana|2.6666666666666665|38.666666666666664|6666666.666666667|\n",
      "|  Juan|               1.0|              33.0|        4500000.0|\n",
      "|Miguel|               0.0|              21.0|        6000000.0|\n",
      "|  Luis|               5.0|              62.0|             NULL|\n",
      "|Daniel|               6.0|              30.0|        9000000.0|\n",
      "+------+------------------+------------------+-----------------+\n",
      "\n",
      "DataFrame promediando una variable específica\n",
      "\n",
      "+------+-----------------+\n",
      "|Nombre|     avg(Salario)|\n",
      "+------+-----------------+\n",
      "|   Ana|6666666.666666667|\n",
      "|  Juan|        4500000.0|\n",
      "|Miguel|        6000000.0|\n",
      "|  Luis|             NULL|\n",
      "|Daniel|        9000000.0|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_avg = emp_act.groupBy(\"Nombre\").avg()\n",
    "emp_avg_sal = emp_act.groupBy(\"Nombre\").agg({\"Salario\":\"avg\"})\n",
    "\n",
    "print(\"DataFrame Inicial\\n\")\n",
    "emp_act.show()\n",
    "\n",
    "print(\"DataFrame promediando todas las variables\\n\")\n",
    "emp_avg.show()\n",
    "\n",
    "print(\"DataFrame promediando una variable específica\\n\")\n",
    "emp_avg_sal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UOeCCN_I3y2"
   },
   "source": [
    "##### orderBy(var)\n",
    "Entrega un DataFrame ordenado de acuerdo a la variable indicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nqZxGoYhI6ll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Inicial\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n",
      "DataFrame ordenado por salario\n",
      "\n",
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_or_sal = emp_act.orderBy(emp_act.Salario.desc())\n",
    "\n",
    "print(\"DataFrame Inicial\\n\")\n",
    "emp_act.show()\n",
    "print(\"DataFrame ordenado por salario\\n\")\n",
    "emp_or_sal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbmHI5hzLFAm"
   },
   "source": [
    "##### select(lista_var)\n",
    "Genera un nuevo DataFrame que incluye únicamente las variables indicadas, es decir, se selecciona un grupo de columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XxmOtgSYLQET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+\n",
      "|Nombre|     Sexo|Salario|\n",
      "+------+---------+-------+\n",
      "|  Juan|Masculino|4500000|\n",
      "|   Ana| Femenino|6200000|\n",
      "|Miguel|Masculino|6000000|\n",
      "|   Ana| Femenino|7600000|\n",
      "|  Luis|Masculino|   NULL|\n",
      "|Daniel|Masculino|9000000|\n",
      "|   Ana| Femenino|6200000|\n",
      "+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_sel = emp_act.select(\"Nombre\",\"Sexo\",\"Salario\")\n",
    "\n",
    "emp_sel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZDEtgcFLPpt"
   },
   "source": [
    "##### selectExpr(expresiones)\n",
    "Funciona similar al método select(), pero permite crear nuevas variables (columnas) a partir de la evaluación de las expresiones indicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "x3FgS9fRMA8X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+-------------------+\n",
      "|Nombre|     Sexo|Salario|(Salario > 6000000)|\n",
      "+------+---------+-------+-------------------+\n",
      "|  Juan|Masculino|4500000|              false|\n",
      "|   Ana| Femenino|6200000|               true|\n",
      "|Miguel|Masculino|6000000|              false|\n",
      "|   Ana| Femenino|7600000|               true|\n",
      "|  Luis|Masculino|   NULL|               NULL|\n",
      "|Daniel|Masculino|9000000|               true|\n",
      "|   Ana| Femenino|6200000|               true|\n",
      "+------+---------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emps = emp_act.selectExpr(\"Nombre\",\"Sexo\",\"Salario\", \"Salario > 6000000\")\n",
    "\n",
    "emps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3_d7U68M9M3"
   },
   "source": [
    "##### withcolumn(nombre,expresion)\n",
    "Permite adicionar columnas con el resultado de la evaluación de una expresión y se puede asignar el nombre de las columnas adicionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OFsNYeL_NPyd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+---------------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|Mayor a 50 años|\n",
      "+---+------+----+---------+----------+-------+---------------+\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|          false|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|          false|\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|          false|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|          false|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|           true|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|          false|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|          false|\n",
      "+---+------+----+---------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_wc = emp_act.withColumn(\"Mayor a 50 años\", emp_act.Edad > 50)\n",
    "\n",
    "emp_wc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpFvpnD1h8lR"
   },
   "source": [
    "# 2. Consultas SQL\n",
    "Spark proporciona una funcionalidad adicional que permite realizar operaciones en DataFrames utilizando SQL, similar a cómo se trabaja con bases de datos relacionales. Esto es posible gracias a la clase SparkContext, que permite ejecutar consultas SQL en DataFrames. \n",
    "\n",
    "En general, el lenguaje SQL nos permite operar con DataFrames en Apache Spark mediante el SparkContext y los métodos descritos anteriormente permiten operar los datos para crear registros, leerlos, actualizarlos e incluso eliminarlos, las operaciones básicas de un CRUD. Como observamos con anterioridad, estos métodos son útiles para manipular y transformar datos de manera eficiente.\n",
    "\n",
    "Para utilizar SQL con DataFrames, primero debes registrar un DataFrame como una \"tabla temporal\" en el motor SQL de Spark. Esto se hace utilizando el método createOrReplaceTempView() de un DataFrame, que permite darle un nombre a la tabla temporal. Una vez registrada, la tabla se puede utilizar en consultas SQL como si fuera una tabla en una base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bwPeNt3vQI7e"
   },
   "outputs": [],
   "source": [
    "emp_drop_id.createOrReplaceTempView(\"Empleados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl5SecKpIsi_"
   },
   "source": [
    "De esta manera cada que requiere utilizar una consulta SQL lo realizará sobre la tabla Empleados\n",
    "\n",
    "**Lectura de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "6kynpGrkI3Am"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  2|   Ana|  38| Femenino|Arquitecta|6200000|\n",
      "|  4|   Ana|  40| Femenino|   Docente|7600000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = spark.sql(\"SELECT * FROM Empleados\")\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E50AvCAQZ88"
   },
   "source": [
    "Filtremos los registros de los empleados masculinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "XvgsfiUOQuSv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------+----------+-------+\n",
      "| Id|Nombre|Edad|     Sexo| Profesion|Salario|\n",
      "+---+------+----+---------+----------+-------+\n",
      "|  0|Miguel|  21|Masculino|      Chef|6000000|\n",
      "|  1|  Juan|  33|Masculino| Ingeniero|4500000|\n",
      "|  5|  Luis|  62|Masculino|  Panadero|   NULL|\n",
      "|  6|Daniel|  30|Masculino|Empresario|9000000|\n",
      "+---+------+----+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2 = spark.sql('SELECT * FROM Empleados WHERE Sexo = \"Masculino\"')\n",
    "q2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0X3x4XSQeWQ"
   },
   "source": [
    "Leamos solo las variables de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zgvKL_tvQZYJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n",
      "|Nombre|Edad|     Sexo|\n",
      "+------+----+---------+\n",
      "|Miguel|  21|Masculino|\n",
      "|  Juan|  33|Masculino|\n",
      "|   Ana|  38| Femenino|\n",
      "|   Ana|  40| Femenino|\n",
      "|  Luis|  62|Masculino|\n",
      "|Daniel|  30|Masculino|\n",
      "+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q3 = spark.sql(\"\"\"\n",
    "\t\tSELECT Nombre, Edad, Sexo \n",
    "\t\tFROM Empleados\n",
    "\t\"\"\")\n",
    "q3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcM47Qz-RdHu"
   },
   "source": [
    "**Funciones definidas por el usuario**\n",
    "\n",
    "Además de las consultas básicas que proporciona SQL, el usuario puede definir sus propias funciones e incorporarlas dentro de una consulta SQL. Esto es de gran ayuda para personalizar las acciones que deseamos realizar sobre los datos. Primero debemos definir la función a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "cww5-ytaRggc"
   },
   "outputs": [],
   "source": [
    "def funcionEdad(edad):\n",
    "\tif edad < 50:\n",
    "\t\treturn \"Menor de 50\"\n",
    "\telse:\n",
    "\t\treturn \"Mayor de 50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fveSTYy6R71d"
   },
   "source": [
    "Ahora debemos registrar la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1ii7JsXTRliD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.funcionEdad(edad)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"funcionEdad\",funcionEdad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i145J-USKyA"
   },
   "source": [
    "Ahora si podemos empezar a usar nuestra función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "umeMyQseSQJn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+-------------+\n",
      "|Nombre|     Sexo|Salario|Edad_Mayor_50|\n",
      "+------+---------+-------+-------------+\n",
      "|Miguel|Masculino|6000000|  Menor de 50|\n",
      "|  Juan|Masculino|4500000|  Menor de 50|\n",
      "|   Ana| Femenino|6200000|  Menor de 50|\n",
      "|   Ana| Femenino|7600000|  Menor de 50|\n",
      "|  Luis|Masculino|   NULL|  Mayor de 50|\n",
      "|Daniel|Masculino|9000000|  Menor de 50|\n",
      "+------+---------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q4 = spark.sql(\"\"\"\n",
    "\t\tSELECT Nombre, Sexo, Salario, funcionEdad(Edad) AS Edad_Mayor_50\n",
    "\t\tFROM Empleados\n",
    "\t\t\"\"\")\n",
    "q4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursos\n",
    "\n",
    "Los siguientes enlaces corresponden a sitios en donde encontrará información muy útil para profundizar en el conocimiento y manejo de dataframes y SQL en Spark:\n",
    "\n",
    "\n",
    "* [Pyspark By Examples](https://sparkbyexamples.com/pyspark-tutorial/)\n",
    "* [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "* [Spark SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "* [Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO6WtSFLk+gU90rhev+Ywt",
   "collapsed_sections": [],
   "name": "07 - DATAFRAME Y SQL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
